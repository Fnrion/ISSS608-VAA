{
  "hash": "6dd5ae071ce78925666d5ea4592dd958",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"In-class_Ex05\"\nauthor: \"Zou Jiaxun\"\ndate: \"May 11, 2024\"\ndata-modified: \"last-modified\"\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  freeze: true\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Click to view code\"\n---\n\nPut all data into one tibular dataframe. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidyverse, readtext,\n               quanteda, tidytext)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_folder <- \"data/MC1/articles\"\n```\n:::\n\nText sensing to extract text\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data <- readtext(paste0(\"data/MC1/articles\",\n                \"/*\"))\n```\n:::\n\nOR\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data <- readtext(\"data/MC1/articles\")\n```\n:::\n\n\nBasic tokenisation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusenet_words <- text_data %>%\n  unnest_tokens(word, text) %>%  #reading the text data\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word) #remove stop words\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nusenet_words %>%\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nreadtext object consisting of 3261 documents and 0 docvars.\n# A data frame: 3,261 × 3\n  word             n text     \n  <chr>        <int> <chr>    \n1 fishing       2177 \"\\\"\\\"...\"\n2 sustainable   1525 \"\\\"\\\"...\"\n3 company       1036 \"\\\"\\\"...\"\n4 practices      838 \"\\\"\\\"...\"\n5 industry       715 \"\\\"\\\"...\"\n6 transactions   696 \"\\\"\\\"...\"\n# ℹ 3,255 more rows\n```\n\n\n:::\n:::\n\n\nObservations- Most common words are: fishing, sustainable, company\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp_table <- usenet_words %>%\n  count(word, sort = TRUE)\n```\n:::\n\n\nCreating a table to observe word counts\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_text <- corpus(text_data)\nsummary(corpus_text, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 338 documents, showing 5 documents:\n\n                                   Text Types Tokens Sentences\n Alvarez PLC__0__0__Haacklee Herald.txt   206    433        18\n    Alvarez PLC__0__0__Lomark Daily.txt   102    170        12\n   Alvarez PLC__0__0__The News Buoy.txt    90    200         9\n Alvarez PLC__0__1__Haacklee Herald.txt    96    187         8\n    Alvarez PLC__0__1__Lomark Daily.txt   241    504        21\n```\n\n\n:::\n:::\n\n\nTo separate the data; with 2 columns X & Y.  Some text are \"1\" hence the split does not occur\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data_splitted <- text_data %>%\n  separate_wider_delim(\"doc_id\",\n                       delim = \"__0__\",\n                       names = c(\"X\", \"Y\"),\n                       too_few = \"align_end\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(jsonlite, tidyverse)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n##pacman::p_load(jsonlite, tidygraph,\n##ggraph, tidyverse, readtext,\n               ##quanteda, tidytext)\n```\n:::\n\n\n\n\nIn the code chunk below, fromJSON() of jsonlite package is used to import *MC3.json* into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmc1_data <- fromJSON(\"data/MC1/mc1.json\")\n```\n:::\n\n\n\nmc2_data <- fromJSON(\"data/MC2/mc2.json\")\n\nmc3_data <- fromJSON(\"data/MC3/mc3.json\")\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}